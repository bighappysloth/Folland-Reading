\documentclass[../main-v2-manifolds.tex]{subfiles}

\begin{document}
%
%
% End of chapter 0%
%
\fchapter{1: Multilinear maps}\newpage
\topheader{Bilinear maps}
\begin{definition}[Bilinear map]
A map $\varphi: E_1\times E_2\to F$, where $F$ is also a Banach space, is said to be \emph{bilinear} if
\[
    \varphi(x,\cdot): E_2\to F\qqtext{and}\varphi(\cdot,y): E_1\to F
\]
are linear for every $x\in E_1$ and $y\in E_2$.     
\end{definition}
\begin{wts}[Continuity criterion of a bilinear map]\label{prop:characterization-of-continuity-bilinear-map}
    Let $E_1$, $E_2$, $F$ be Banach spaces, a bilinear map $m: E_1\times E_2\to F$ is continuous if and only if there exists a $C\geq 0$, where
    \begin{equation}\label{eq:continuity-bilinear-map}
        \vert m(x,y)\vert\leq C\vert x\vert\vert y\vert    
    \end{equation}
\end{wts}
\begin{proof}
    Suppose such a $C$ exists, fix a convergent sequence $(x_n, y_n)\to (x,y)$ in $E_1\times E_2 = E$. Because the projection maps are continuous, this means $x_n\to x$ and $y_n\to y$. Using inspiration from the proof where $x_ny_n\to xy$, where
    \[
        x_n(y_n-y) + (x_n-x)y = x_ny_n- xy\quad x,y,x_n,y_n\in\real
    \]
    Using the inspiration, and replacing multiplication in $\real$ with the bilinear map $m$, we have:
    \begin{align*}
        m(x_n,\: y_n-y) + m(x_n-x,\: y) &= m(x_n,\:y_n) - m(x,\:y)\\
        \vert m(x_n,\: y_n) - m(x,\: y)\vert &\leq C[\vert x_n\vert\cdot\vert y_n - y\vert + \vert x_n - x\vert\cdot\vert y\vert]\to 0
    \end{align*}

    Conversely, if $m$ is continuous, then it is continuous at the origin $(0,0)=0$. There exists a $\delta$ where $\vert (x,y)\vert \leq \delta$ implies $\vert m(x,y)\vert \leq 1$. Now, if $x,y\neq 0$ are elements in $E$, we normalize so that $(x,y)$ has length $\delta$

    \[
        \vert(x\vert x\vert^{-1}\delta,y\vert y\vert^{-1}\delta)\vert = \delta\vert(x\vert x\vert^{-1},y\vert y\vert^{-1})\vert = \delta
    \]

    So that $\vert m(x\vert x\vert^{-1}\delta, y\vert y\vert^{-1}\delta)\vert \leq 1$, using bilinearity of $m$:
    \[
        \vert m(x,y)\vert\leq \delta^{-2}\vert x\vert\cdot\vert y\vert
    \]

    Setting $\delta^{-2} = C$ finishes the proof (notice if eithe $x$ or $y$ is $0$, then $m$ is trivially $0$ and the inequality holds).
\end{proof}


\begin{wts}[$L(E_1,E_2;F)$ is isomorphic to $L(E_1, L(E_2,F))$]\label{prop:bilinear-map-isomorphism-currying}
    For each bilinear map $\omega\in L(E_1,E_2;F)$, there exists a unique map $\varphi_\omega\in L(E_1, L(E_2,F))$ such that $\vert \omega\vert = \vert\varphi_\omega\vert$; such that for every $(x,y)\in E_1\times E_2$, $\omega(x,y) = \varphi(x)(y)$.
\end{wts}
\begin{proof}
    Let $\varphi_\omega: E_1\to L(E_2,F)$ be the unique map such that $\varphi_\omega(x)(y) = \omega(x,y)$. \Cref{prop:characterization-of-continuity-bilinear-map} shows that $\varphi_\omega(x)$ is a continuous linear map into $F$ at each $x$, and $\vert\varphi_\omega(x)\vert\leq\vert\omega\vert\vert x\vert$. This holds for an arbitrary $x$, and $\varphi_{\omega}(\cdot)$ is clearly linear, hence $\vert\varphi_\omega\vert\leq\vert\omega\vert$. Reversing the roles of $\omega$ and $\varphi$ shows proves the other estimate.\\

    The rule as outlined above is linear in $\omega$; and it is not hard to see $\varphi: L(E_1,E_2; F)\to L(E_1, L(E_2, F))$ is an injection. By the open mapping theorem, the proposition is proven if $\varphi$ is a surjection. Fix $\theta\in L(E_1, L(E_2,F))$, define a map $\omega: E_1\times E_2\to F$ such that $\omega(x,\cdot) = \theta(x)(\cdot)$. So that $\omega$ is linear in its second argument. To show $\omega$ is linear in its first: fix a linear combination $A = \sum^\wedge x$ in $E_1$, and $y\in E_2$. 
    \[
    \omega(A,y) =\theta(\sum^\wedge x)(y) = \sum^\wedge \theta(x)(y) = \sum^\wedge \omega(x,y)
    \]
    Continuity follows from \Cref{eq:continuity-bilinear-map}, and $\varphi_\omega = \theta$ as needed.
\end{proof}

\topheader{$k$-linear maps}
\begin{definition}[$k$-linear maps]\label{def:k-linear-maps}
    Let $\UL{E}[k]$, $F$ be Banach spaces. A map $\varphi: \prod \UL{E}[k]$ is $k$-linear if for every $i=\underline{k}$, $v_i\in E_i$, 
    \[
        \varphi(\UPL{\cdot}[i-1],v_i,\UPL{\cdot}[k-i]):\: \bigoprod (\UL{E}[i-1],\UL{E}[i+][k-i])\to F\quad\text{is }(k-1)\text{-linear}
    \]
\end{definition}
%%%
A $k$-linear \emph{symmetric} map between Banach spaces $E,F$ is a map $A\in \mathcal{L}(E^k,F)$ such that for every $k$-permutation $\theta\in S_{\underline{k}}$, 
\[
    A(\Ul{v}[k]) = A(\Ul{v}[\theta(k)])    
\]

%%%

The following theorem should give confidence to the notation we have adopted to use.
\begin{wts}[Continuity criterion of $k$-linear maps]
    Let $\UL{E}[k]$ and $F$ be Banach spaces, a $k$-linear map $\varphi: \prod \UL{E}[k]\to F$ is continuous iff there exists a $C>0$, such that for every $x_i\in E_i$, $i=\underline{k}$
    \[
        \abs{\varphi(\UL{x}[k])} \leq C\prod \abs{\UL{x}[k]}
    \]
\end{wts}
\begin{proof}
    Suppose $\varphi$ is continuous, then it is continuous at the origin. Picking $\varepsilon = 1$ induces a $\delta>0$ such that for $\abs{(\UL{x}[k])}\leq \delta$, $\abs{\varphi(\UL{x}[k])}\leq 1$. The usual trick of normalizing an arbitrary vector $(\UL{x}[k])\in \prod \UL{E}[k]$ does the job:
    \[
        \abs{\varphi(x_k\cdot \abs{\UL{x}[k]}^{-1}\cdot\delta)}\leq 1\implies \abs{\varphi(\UL{x}[k])}\leq \delta^{-k}\prod\abs{\UL{x}[k]}
    \]
    Conversely, fix a sequence (indexed by $n$, in $k$ elements in the product space $\prod\UL{E}[k]$), so
    \begin{equation}\label{eq:k-linear-sequence-def}
        (x_n^{\underline{k}})\to (x^{\underline{k}})\quad \text{as } n\to +\infty
    \end{equation}
    To proceed any further, we need to prove an important equation that decomposes a difference in $\varphi$.
    \begin{equation}\label{eq:k-linear-lemma-1}
        \varphi(\UPL{b}[k]) - \varphi(\UPL{a}[k]) = \sum_{i=\underline{k}}\varphi(\UPL{b}[i-1],\Delta_i, \UPL{a}[i+][k-i])
    \end{equation}
    where $(\UPL{b}[k])$ and $(\UPL{a}[k])$ are elements in $\prod \UL{E}[k]$, and $\Delta_i = b^i - a^i$ for $i = \underline{k}$. The proof is in the following note, which is in more detail than usual - to help the reader ease into the new notation.
    \begin{note}
        We proceed by induction, and \cref{eq:k-linear-lemma-1} follows by setting $m=k$ in
        \begin{equation}\label{eq:k-linear-lemma-2}
            \varphi(\UPL{a}[k]) = \varphi(\UPL{b}[m],\UPL{a}[m+][k-m]) - \sum_{i=\underline{m}}\varphi(\UPL{b}[i-1],\Delta_i,\UPL{a}[i+][k-i])
        \end{equation}
        Base case: set $m=1$, by definition of $k$-linearity (\cref{def:k-linear-maps}) of $\varphi$. Since $a^1 = b^1-\Delta_1$, 
        \[
            \varphi(\UPL{a}[k]) = \varphi(b^1-\Delta_1, \UPL{a}[1+][k-1]) = \varphi(b^1, \UPL{a}[1+][k-1]) - \varphi(\Delta_1,\UPL{a}[1+][k-1])
        \]
        Induction hypothesis: suppose \cref{eq:k-linear-lemma-2} holds for a fixed $m$. Since $a^{m+1} = b^{m+1} - \Delta_{m+1}$, 
        \begin{align*}
            \varphi(\UPL{a}[k]) &= \varphi(\UPL{b}[m],\UPL{a}[m+][k-m]) - \sum_{i=\underline{m}}\varphi(\UPL{b}[i-1],\Delta_i,\UPL{a}[i+][k-i])\\
            &= \varphi(\UPL{b}[m],a^{m+1},\UPL{a}[(m+1)+][k-(m+1)]) - \sum_{i=\underline{m}}\varphi(\UPL{b}[i-1],\Delta_i,\UPL{a}[i+][k-i])\\
            &= \varphi(\UPL{b}[m+1],\UPL{a}[(m+1)+][k-(m+1)]) - \varphi(\UPL{b}[m+1],\Delta_{m+1},\UPL{a}[(m+1)+][k-(m+1)]) - \sum_{i=\underline{m}}\varphi(\UPL{b}[i-1],\Delta_i,\UPL{a}[i+][k-i])
        \end{align*}
        and this proves \cref{eq:k-linear-lemma-1}
    \end{note}
    We substitute $a^i = x^i$, and $b^i = x_n^i$ for $i = \underline{k}$, and \cref{eq:k-linear-lemma-1} becomes \cref{klinear-eq}
    \begin{equation}\label{klinear-eq}
        \varphi(x_n^{\underline{k}}) - \varphi(x^{\underline{k}}) = \sum_{i=\underline{k}}\varphi(x_n^{\underline{i-1}}, x_n^i - x^i, x^{i+\underline{k-i}})
    \end{equation}
    Then the triangle inequality reads
    \begin{align*}
        \abs{\varphi(x_n^{\underline{k}}) - \varphi(x^{\underline{k}})} &\leq \sum_{i=\underline{k}} \abs{\varphi(x_n^{\underline{i-1}}, x_n^i - x^i, x^{i+\underline{k-i}})}\leq \sum_{i=\underline{k}}\abs{\varphi}\cdot\bigoprod\qty({\UPL{x_n}[i-1],\Delta_i,\UPL{x}[i+][k-i]})\\[1ex]
        &\leq \sum_{i=\underline{k}}\abs{\varphi}\cdot\abs{x_n^i - x^i}\bigoprod\qty(x_n^{\underline{i-1}}, x^{i+\underline{k-i}})\Lsim_n\vert\varphi\vert\sup_{i=\underline{k}}\vert x_n^i - x^i\vert\to 0
    \end{align*}
    where we identify the product $\bigoprod\qty(\UPL{v}[k])$ with the product of their norms $\bigoprod \qty(\abs{\UPL{v}[k]})$. 
\end{proof}
\begin{remark}[Currying isomorphism]
    The $k$-linear variant of \cref{prop:bilinear-map-isomorphism-currying} holds. We will use but not prove this fact.
\end{remark}
\begin{remark}[$k$-linear maps from the same space]
We denote the space of $k$-linear maps from $E$ into $F$ by $L(\UL{E}[k]; F) = L(E^k, F) = L^k(E,F)$. \emph{Tensors} on $E$ are $k$-linear maps from the product space of $E$ into $\real$, by replacing $F$ with $\real$.
\end{remark}
%
% What to include also?
% Should we start from defining the properties of Banach Spaces? TVS?
%

\fchapter{2: Differentiation}\newpage
\topheader{The derivative}
\begin{definition}[Open sets and neighbourhoods]\label{def:osub-notation}
    If $U$ is an open subset of a topological space $X$, we denote this by $U\osub X$. If $U$ is a \emph{neighbourhood} of a point $p\in X$, we write $p\oin U$. \\
    
    We do not require neighbourhoods to be open sets; rather, we say $U$ is a neighbourhood of $p$ when the interior of $U$ contains $p$.
\end{definition}
\begin{definition}[Little $o$]\label{def:little-oh}
    A real-valued function in a real variable defined for all $t$ sufficiently small is said to be \emph{$o(t)$} if $\lim_{t\to 0}o(t)/t=0$. A map $\psi: U\to  F$ where $U\osub E$ contains $0$ in $E$, is said to be $o(h)$ if $\vert \psi(h)\vert/\vert h\vert \to 0$ as $h\to 0$ in $E$.
\end{definition}
\begin{definition}[Differentiability]\label{def:differentiability}
    Let $f: E\to F$ be a map, replacing $E$ and $F$ by their open subsets if necessary. We say $f$ is \emph{differentiable} at $x\in E$ when there exists a \textbf{continuous linear map on $E$}: $\lambda\in L(E,F)$ such that
    \begin{equation}\label{eq:differentiability}
        f(x+h) = f(x) + \lambda h + o(h)\quad\text{for sufficiently small }h
    \end{equation}
    The role $o(h)$ plays here is a map from $U\to F$, where $U$ is some neighbourhood of $0$. 
\end{definition}
\begin{wts}[Basic properties of the derivative]\label{prop:basic-properties-of-derivative}
    If $f$ is differentiable at $x$, then the $\lambda$ in \cref{eq:differentiability} is unique. We write $f'(x) = Df(x) = \lambda$ as in \cref{eq:differentiability-def}. Furthermore, if $f'(x)$ and $g'(x)$ exist, then $(f+g)'(x) = f'(x) + g'(x)$ as linear maps, similar for scalar multiplication.
\end{wts}
\begin{proof}
    Suppose $\lambda_i\in L(E,F)$ are both derivatives of $f$ at $x$. Then,
    \[
        \begin{cases}
        f(x+h) = f(x) + \lambda_1(h) + o(h)\\
        f(x+h) = f(x) + \lambda_2(h) + o(h)
        \end{cases}
    \]
    And $(\lambda_1 - \lambda_2)(h) = o(h) = \varphi(h)\cdot\abs{h}$, where $\varphi(h)\to 0$ as $h\to 0$. Using the operator norm, we see that
    \[
        \norm{\lambda_1-\lambda_2}_{L(E,F)}\leq \abs{\varphi(h)}\to 0
    \]
    This proves uniqueness. Suppose $f$ and $g$ are differentiable at $x$, denote $\lambda_f = f'(x)$ (resp. $g'(x)$). The definition of \cref{def:differentiability} reads
    \begin{align}
        f(x+h) + g(x+h) &= \qty(f(x) + g(x)) + \qty(\lambda_f(h) + \lambda_g(h)) + o(h) + o(h)\nonumber\\
        (f + g)(x+h) &= (f+g)(x) + (\lambda_f + \lambda_g)(h) + o(h)\label{eq:differentiable-addition-eq}
    \end{align}
    since \cref{eq:differentiable-addition-eq} satisfies \cref{eq:differentiability}, the proof is complete.
\end{proof}
\begin{wts}[Chain rule]\label{prop:chain-rule}
    Let $E, F, G$ be Banach spaces. If $f\in C^1(E,F)$, $g\in C^1(F,G)$, for every $x\in E$,
    \begin{equation}\label{eq:chain-rule}
        (g\circ f)'(x) = g'(f(x))\circ f'(x)
    \end{equation}
\end{wts}
\begin{proof}
    Since $f$ is differentiable at $x$, $f(x+h) = f(x) + f'(x)(h) + o_1(h)$, (resp. for $g$, $o_2(h)$). Set $k(h) = f(x+h) - f(x)$, and 
    \begin{align*}
        g(f(x+h)) &= g(f(x)) + g'(f(x))(k(h)) + o_2(k(h))\\
        &= g(f(x)) + g'(f(x))(f'(x)(h) + o_1(h)) + o_2(k(h))\\
        (g\circ f)(x+h) &= (g\circ f)(x) + g'(f(x))\circ f'(x)(h) + g'(f(x))(o_1(h)) + o_2(k(h))\\
        (g\circ f)(x+h) &= (g\circ f)(x) + g'(f(x))\circ f'(x)(h) + o(h)
    \end{align*}
    because $\abs{A(o_1(h))}\leq \abs{A}\abs{o_1(h)}$ for all $A\in L(E,F)$; and $o(k(h)) = o(h)$ for every continuous $k: E\to F$ such that $k(h)\to 0$ as $h\to 0$.
\end{proof}
\begin{wts}[Derivatives of CLMs]\label{prop:differentiating-linear-maps}
    If $\lambda\in L(E,F)$, then $\lambda\in C^1(E,F)$ and $D\lambda(x) = \lambda$ for every $x\in E$. Furthermore, if $f\in C^1(E,F)$, and $\nu\in L(F,G)$, then the composition $\nu\circ f$ is in $C^1(E,G)$, and $(\nu\circ f)'(x) = \nu\circ f'(x)$ for every $x\in E$.
\end{wts}
\begin{proof}
    See $\lambda(x+h) = \lambda(x) + \lambda(h) + 0$ at every $x\in E$. Using the chain rule (\cref{prop:chain-rule}) proves the second claim.
\end{proof}
\begin{wts}[Product rule in $k$ variables]\label{prop:product-rule-k-variables}
    Let $m: \prod \UL{F}[k]\to G$ be a continuous $k$-linear map between Banach spaces $\UL{F}[k]$ and $G$. Suppose $f_i\in C^1(E, F_i)$ with $i=\underline{k}$, writing 
    \begin{equation}\label{eq:k-linear-multiplication-map-def}
        m(\UL{f}[k])(x) = m(\UL{f}[k](x))
    \end{equation}
    then $m(\UL{f}[k])$ is in $C^1(E,G)$ and for every $y\in E$,
    \begin{equation}\label{eq:k-linear-product-rule}
        Dm(\UL{f}[k])(x)(y) = \sum_{i=\underline{k}}m(\UL{f}[i-1](x),Df_i(x)(y),\UL{f}[i+][k-i](x))
    \end{equation}
\end{wts}
\begin{proof}
    Let $x$ be fixed. \Cref{eq:k-linear-product-rule} is proven if we show \cref{eq:product-rule-k-variables-modified-equation}
    \begin{equation}\label{eq:product-rule-k-variables-modified-equation}
        m(\UL{f}[k])(x+h) = m(\UL{f}[k])(x) + \qty(\sum_{i=\underline{k}}m(\UL{f}[i-1](x), Df_i(x)(h),\UL{f}[i+][k-i](x))) + o(h)
    \end{equation}
    and for sufficiently small $h$ we have
    \begin{equation}\label{eq:product-rule-k-variables-o(h)}
        f_i(x+h) - f_i(x) = Df_i(x)(h) + o(h^i)
    \end{equation}
    We will use the difference formula in \cref{eq:k-linear-lemma-2}, with the following substitutions
    \begin{align}
        f_i(x+h) &= b^i & f_i(x) &=a^i\label{eq:product-rule-k-variables-sub-1}\\
        Df_i(x)(h) &= c^i &  o(h^i)&=\varepsilon^i\label{eq:product-rule-k-variables-sub-2}\\
        f_i(x+h) - f_i(x) &= c^i + \varepsilon^i & \Delta^i &= o(h^i) + c^i\label{eq:product-rule-k-variables-sub-3}
    \end{align}
    With these substitutions, the equation we want to prove (\cref{eq:k-linear-product-rule}) becomes \cref{eq:product-rule-k-variables-modified-k-linear}
    \begin{equation}\label{eq:product-rule-k-variables-modified-k-linear}
        m(\UPL{b}[k]) - m(\UPL{a}[k]) = \qty(\sum_{i=\underline{k}}m(\UPL{a}[i-1],c^i,\UPL{a}[i+][k-i])) + o(h)
    \end{equation}
    Starting from \cref{eq:k-linear-lemma-2}, 
    \[
        m(\UPL{b}[k]) - m(\UPL{a}[k]) = \sum_{i=\underline{k}}m(\UPL{b}[i-1],\Delta^i,\UPL{a}[i+][k-i])
    \]
    We can expand each term, if $i=\underline{k}$, 
    \begin{equation}\label{eq:product-rule-k-variables-expand-1}
        m(\UPL{b}[i-1],\Delta^i,\UPL{a}[i+][k-i]) = m(\UPL{b}[i-1],c^i,\UPL{a}[i+][k-i])+ m(\UPL{b}[i-1],o(h^i),\UPL{a}[i+][k-i])
    \end{equation}
    Let us study the first term in \cref{eq:product-rule-k-variables-expand-1}, and with $i$ held fixed, define
    \begin{equation}\label{eq:product-rule-k-variables-m_i-function}
        m_i(\UPL{z}[i-1]) = m(\UPL{z}[i-1],c_i,\UPL{a}[i+][k-i])
    \end{equation}
    Expanding the first term within \cref{eq:product-rule-k-variables-expand-1}, and because $m_i$ as defined in \cref{eq:product-rule-k-variables-m_i-function} is $i-1$-linear (because it is a $k$-linear map with $k-(i-1)$ variables held constant); we use \cref{eq:k-linear-lemma-2} again.
    \begin{equation}\label{eq:product-rule-k-variables-expand-2}
        m_i(\UPL{b}[i-1]) = \qty(\sum_{i=\underline{k}} m_i(\UPL{b}[j], \Delta^j, \UPL{a}[j+][(i-1)-j])) + m_i(\UPL{a}[i-1])
    \end{equation}
    Unboxing the last term in \cref{eq:product-rule-k-variables-expand-2} using the definition of $m_i$ reads
    \begin{equation}\label{eq:product-rule-k-variables-expand-3}
        m(\UPL{b}[i-1],\Delta^i,\UPL{a}[i+][k-i]) = m(\UPL{a}[i-1],c^i,\UPL{a}[i+][k-i]) + \sum_{j=\underline{i-1}}m_i(\UPL{b}[j],\Delta^j,\UPL{a}[j+][(i-1)-j])
    \end{equation}
    We wish to remove all of the $b^i$s. Since $\Delta^i = c^i + \varepsilon^i$ (\cref{eq:product-rule-k-variables-sub-3}), we have
    \begin{align}
        m(\UPL{b}[k]) - m(\UPL{a}[k]) &= \sum_{i=\underline{k}} m(\UPL{b}[i-1],c^i,\UPL{a}[i+][k-i]) + m(\UPL{b}[i-1],\varepsilon^i,\UPL{a}[i+][k-i])\nonumber\\
        &= \qty(\sum_{i=\underline{k}}m_i(\UPL{b}[i-1])) + \sum_{i=\underline{k}}m(\UPL{b}[i-1],\varepsilon^i,\UPL{a}[i+][k-i])\nonumber\\
        &= \qty(\sum_{i=\underline{k}} m_i(\UPL{a}[i-1]) + \sum_{j=\underline{i-1}}m_i(\UPL{b}[j-1],\Delta^{j},\UPL{a}[j+][(i-1)-j])) + \sum_{i=\underline{k}} m(\UPL{b}[i-1],\varepsilon^i,\UPL{a}[i+][k-i])\nonumber\\
        &= \qty(\sum_{i=\underline{k}}m_i(\UPL{a}[i-1]))  + \sum_{\substack{i=\underline{k}\\ j=\underline{i-1}}} m_i(\UPL{b}[j-1],\Delta^j,\UPL{a}[j+][(i-1)-j]) + \sum_{i=\underline{k}}m(\UPL{b}[i-1], \varepsilon^i,\UPL{a}[i+][k-i])\label{eq:product-rule-k-variables-expand-4}
    \end{align}
    The last term within \cref{eq:product-rule-k-variables-expand-4} is $o(h)$, since it is a linear combination of $o(h^i)$s. 
    \begin{equation}\label{eq:product-rule-k-variables-last-term-o(h)}
    \abs{\sum_{i=\underline{k}}m(\UPL{b}[i-1], \varepsilon^i,\UPL{a}[i+][k-i])}\Lsim_{m,a,b}\vert o(h)\vert
    \end{equation}
    Each summand in the second last term in \cref{eq:product-rule-k-variables-expand-4} is $o(h)$ as well, as
    \begin{align}
        \abs{m_i(\UPL{b}[j-1],\Delta^j,\UPL{a}[j+][(i-1)-j])}&\leq\vert m_i\vert\bigoprod(\UPL{b}[j-1],\Delta^j,\UPL{a}[j+][(i-1)-j])\\[1ex]
        &\leq \abs{m}\cdot\qty(\bigoprod(c^i,\UPL{a}[i+][k-i]))\qty(\bigoprod(\UPL{b}[j-1],\Delta^j,\UPL{a}[j+][(i-1)-j]))\\[1ex]
        &\Lsim_{m,a,b}\sup_{\substack{i=\underline{k}\\ j = \underline{i-1}}}\abs{c^i}\cdot\abs{\Delta^j}\nonumber\\[1ex]
        &\Lsim_{m,a,b}\sup_{\substack{i=\underline{k}\\ j = \underline{i-1}}}\abs{Df_i(x)(h)}\cdot\abs{f_j(x+h) - f_j(x)}\nonumber\\[1ex]
        &\Lsim_{m,a,b}\abs{Df_i(x)}\abs{h}\sup_{\substack{i=\underline{k}\\ j=\underline{i-1}}}\abs{\Delta^j}\Lsim_{m,a,b}\abs{o(h)}\label{eq:product-rule-k-variables-second-last-term-o(h)}
    \end{align}
    for the second last estimate we used $\Delta^j\to 0$ as $h\to 0$. The second term in \cref{eq:product-rule-k-variables-expand-4} is $o(h)$, and \cref{eq:product-rule-k-variables-modified-equation} is proven. Therefore $m(\UL{f}[k])$ is differentiable at $x$. Continuity of $Dm(\UL{f}[k])$ follows from the fact that 
    \begin{equation}\label{eq:product-rule-k-variables-continuity-0}
        Dm(\UL{f}[k])(x) = \sum_{i=\underline{k}}m(\UL{f}[i-1](x),Df_i(x)(\cdot),\UL{f}[i+][k-i](x))
    \end{equation}
    and each of the summands \cref{eq:product-rule-k-variables-continuity-0} can be broken down as the product of the compositions shown in \cref{eq:product-rule-k-variables-continuity-1,eq:product-rule-k-variables-continuity-2}
    \begin{align}
        x\mapsto(\UL{f}[i-1](x),\UL{f}[i+][k-i](x))\mapsto m(\UL{f}[i-1](x),\cdot,\UL{f}[i+][k-i](x))\label{eq:product-rule-k-variables-continuity-1}\\
        x\mapsto Df_i(x)(\cdot)\label{eq:product-rule-k-variables-continuity-2}
    \end{align}
    which are continuous from $E$ to $L(E,F)$.
\end{proof}
\begin{corollary}[Higher order product rule]
    Let $m: \prod F_{\underline{k}}\to G$ be a continuous $k$-linear map between $F_{\underline{k}}$ and $G$. Suppose $f_i\in C^p(E, F_i)$ for $i = \underline{k}$. Then $m(f_{\underline{k}})$ is in $C^p(E,G)$ as defined in \cref{eq:k-linear-multiplication-map-def}, and 
    \begin{equation}
        D^p m(f_{\underline{k}})(x) = \sum_{\abs{\alpha}=p}\qty(\frac{p!}{\alpha !})m(D^{\alpha_{i=\underline{k}}}f_i(x))
        \label{eq:higher-order-k-linear-product-rule}
    \end{equation}
    where $D^0 f_i(x) = f_i(x)$, $\alpha$ is a $k$ multi-index with entries $\alpha_i\geq 0$ and 
    \[
        \alpha! = \prod_{i=\underline{k}}\alpha_i !
    \]
\end{corollary}

\fchapter{3: Integration}\newpage
\topheader{Introduction}
This chapter will be on the integration of \emph{regulated} mappings, the space of which are precisely the uniform closure of rectangle functions. from a compact interval. We will go through some of the elementary results, and prove the Fundamental Theorem.
\topheader{Integration of step mappings}
\begin{definition}[Partition on {$[a,b]$}]
Let $I=[a,b]$ be a compact interval. An  $N$-\emph{partition} $P$ on $I$ is a list of $N+1$ elements in $[a,b]$, which are assumed to be well ordered as in $p_0\leq p_1\leq\cdots\leq p_N$. 
\begin{equation}\label{eq:partition-def}
    P = (a = p_0,p_1,\ldots,p_N = b)\qqtext{or} P = (p_0,\UL{p}[N])
\end{equation}
The space of partitions on $I$ will be denoted by $I_p$.
\end{definition}
As per usual, we have \emph{common refinements of partitions}, given two partitions $P$ and $Q$ on the same compact interval $I=[a,b]$, where $P$ is defined as in \cref{eq:partition-def}, and $Q = (q_0,\UL{q}[N])$ similarly. The common refinement of $P$ and $Q$ is another partition $R$ on $I$ which contains all of the elements in $P\cup Q$. 
\begin{itemize}
    \item Given a partition \( P \) of size \( N \) represented as \( P = (p_0, \UL{p}[N]) \), the cells of \( P \) are indexed using their rightmost points.
    \item The interval \((p_{i-1}, p_i)\) is denoted as \(\cell(p_i)\), and 
    \item the \emph{length} of the $i$th cell: $\abs{\cell{p_i}}=\abs{p_i - p_{i-1}}$.
    \item If we want to sequence the cells of \( P \) based on their right endpoints, it is expressed as \( \cell(P) = \qty(\cell(\UL{p}[N])) \).
    
    \item Note that these cells do not form a disjoint union of $I$.
\end{itemize}
\begin{remark}[Assume all intervals are compact]
For the rest of this chapter, we assume all intervals are compact and of the form $I = [a,b]$. If $P$, $Q$, $R$ are partitions, their elements will be represented by $p_i$, (resp. $r_i$, $q_i$).
\end{remark}

\begin{definition}[Step mapping]\label{def:step-mapping}
A step mapping on $I = [a,b]$ is a vector space of maps from $I$ to a Banach space $E$ over $\real$. It is equipped with the supremum norm, and its elements are denoted by $\Sigma$,
\begin{equation}\label{eq:step-mapping}
    \Sigma = \bigset{f: [a,b]\to E,\: \text{there exists a $N$-partition }P\in I_p,\:\{\UL{v}[N]\}\subseteq E\text{ such that } f\vert_{(p_{i-1}, p_i)} = v_i\: \forall i = \underline{N}}
\end{equation}
I $f\in \Sigma$, we denote its norm by $\norm{f}_u = \sup_{x\in I}\abs{f(x)}$.
\end{definition}
%With the domain $I$ held fixed, we now define the integral on step mappings, it is easy to show that \cref{def:integral-step-mappings} is independent of the partition chosen.
%
\begin{definition}[Integration on $\Sigma$]\label{def:integral-step-mappings}
    If $f\in \Sigma$ and is of the form inside the set-builder notation in \cref{eq:step-mapping}, we define the integral of $f$ by
    \begin{equation}\label{eq:integral-step-mappings}
        \int_a^b f = \sum_{i=\underline{N}}(p_i - p_{i-1})v_i
    \end{equation}
\end{definition}
%
\begin{remark}[Distinguishing between intervals $I$, $J$]
If $I$ and $J$ are compact intervals, we distinguish the step mappings from $I$ and $J$ by $\Sigma_I$ and $\Sigma_J$.    
\end{remark}
%
We now state some definition and properties of \cref{eq:integral-step-mappings} which we will not prove. 
\begin{wts}[Properties of the integral on $\Sigma$]\label{prop:properties-integral-step-mappings}
    Let $I$ and $J$ be intervals, $f,\UL{f}[k]\in \Sigma_I$, and $g\in \Sigma_J$. 
    \begin{itemize}
        \item The integral is linear, that is
        \begin{equation}\label{eq:integral-step-mappings-linear}
            \int \sum^\wedge \UL{f}[k] = \sum^\wedge \int \UL{f}[k]
        \end{equation}
        \item The integral over $[b,a]$ is \emph{defined} to be the negative of \cref{eq:integral-step-mappings}:
        \begin{equation}\label{eq:integral-step-mappings-negative}
            \int_a^b f = - \int_b^a f
        \end{equation}
        \item The integral is domain-additive, if $b=c$, then 
        \begin{equation}\label{eq:integral-step-mappings-domain-additive}
            \int_a^b f + \int_c^d g = \int_a^d (f+g)
        \end{equation}
        where we identify $(f+g)$ to be the step mapping in $\Sigma_{[a,d]}$ whose restriction $I$ (resp. $J$) agree with $f$ (resp. $g$).
    \end{itemize}
\end{wts}
% Finish introduction to step mappings, now extend by continuity to the space of regulated mappings.
% Product of step mappings.
\topheader{Product of step mappings}
Let $\UL{E}[k]$ be Banach spaces, and $I = [a,b]$ a fixed compact interval. Let $E$ refer  to the product space $\prod \UL{E}[k]$, which is equipped with the supremum norm as outlined in \cref{def:finite-product-banach-space}
\[
    \Sigma_i = \bigset{f_i:I \to E_i,\: f_i \text{ is a step mapping.}}
\]
There are two ways of defining the space of step-mappings from $I$ into $E$ \cref{eq:finite-product-step-mapping-def-1,eq:finite-product-step-mapping-def-2}. Using a combinatorical argument with common refinements, it is not hard to see the two are subsets of each other.
\begin{align}
    \Sigma_{E}^1 &= \bigset{f:I\to E,\: \proj_i f\in \Sigma_i\:\forall i = \underline{k}}\label{eq:finite-product-step-mapping-def-1}\\
    \Sigma_{E}^2 &= \bigset{f:I\to E,\: f \text{ is a step mapping.}}\label{eq:finite-product-step-mapping-def-2}
\end{align}
% Add the proof the above using double inclusion
% Proof for toplinear isomorphism with its external direct sum?

And since the product space $E$ is toplinearly isomorphic to its external direct sum, $E_1\times \cdots\ E_k$, the integral over $\Sigma_{E} = \Sigma_{E}^1 = \Sigma_E^2$ is defined to be 
\begin{equation}\label{eq:finite-product-integration-product-space}
    \int_a^b f = \qty(\int_a^b\UL{\proj}[k]f) = \qty(\int_a^b\proj_1 f,\ldots,\int_a^b\proj_k f)
\end{equation}


\topheader{Regulated mappings}
\begin{definition}[Regulated mappings]\label{def:regulated-mappings}
    Let $I$ be a compact interval. A mapping from $I$ into $E$ is \emph{regulated} if it is the uniform limit of step mappings. We denote the space of regulated mappings by $\cl{\Sigma_I}$ or $\cl{\Sigma}$.
\end{definition}
\begin{wts}[Continuity implies a regulated mapping]\label{prop:continuity-implies-regulated-mapping}
    Every continuous function $f: I\to E$ is the uniform limit of step mappings in $\Sigma_I = \Sigma$.
\end{wts}
\begin{proof}
    Let $f\in C(I, E)$, the continuity of $f$ is uniform; given $\varepsilon>0$ there exists $\delta>0$ where $\abs{y-x}<\delta$ implies $\abs{f(y)-f(x)}<\varepsilon$. $\delta$ induces a smallest integer $n\geq 1$ such that $p_{n}=a + n\delta > b$. Define $p_0 = a$ and $p_i = a + i\delta$, relabelling $p_{n} = b$, we see that $P = (p_0,\UL{p}[n])$ is a partition.\\

    We construct a step mapping by sampling values of $f$. Set $g\vert_{\cell(p_i)}=f(p_{i})$, $g(a) = f(a)$, $g(p_i) = f(p_i)$. Defining the endpoints is necessary, and $g$ still remains a member of $\Sigma_I$ by \cref{eq:step-mapping}. Each $x\in I\setminus P$ belongs in some $\cell(p_i)$, of which $\abs{p_i - x}<\delta$, and $g(x)=f(p_i)$ implies $\abs{g(x)-f(x)}<\delta$. If $x$ is in $P$, then $g(x) = f(x)$, and $\norm{f-g}_{u}\leq+\varepsilon$.
\end{proof}
\begin{wts}[Integration of regulated mappings]\label{prop:continuous-extension-integral-to-regulated-mappings}
    Let $f: I \to E$ be continuous, if $\{f_n\}\subseteq \Sigma$ converges uniformly to $f$, then $\{\int_a^b f_n\}$ is Cauchy in $E$, whose limit we \emph{define} to be $\int_a^b f$ --- the integral of $f$. Furthermore, 
    \begin{enumerate}
        \item For any regulated mapping $f: I\to E$,
        \begin{equation}\label{eq:regulated-mapping-integral-continuous}
            \abs{\int_a^b f}\leq \int_a^b\abs{f} \leq (b-a)\norm{f}_u
        \end{equation}
        \item The integral on $\cl{\Sigma}$  (resp. $\cl{\Sigma_I}$, $\cl{\Sigma_J}$) satisfies all of the properties in \cref{prop:properties-integral-step-mappings}.
    \end{enumerate}
\end{wts}
\begin{proof}
    Let $f$ be a step mapping on $E$, we wish to show \cref{eq:regulated-mapping-integral-continuous} holds. If $f$ is induced by some $n$-partition $P$, 
    \begin{equation}\label{eq:continuous-extension-integral-to-regulated-mappings-eq-1}
    \int_a^b f = \sum_{i=\underline{n}}\abs{\cell(p_i)} f(p_i)\leq \sum_{i=\underline{n}}\abs{\cell(p_i)}\abs{f(p_i)} = \int_a^b\vert f\vert 
    \end{equation}
    The integral in \cref{eq:continuous-extension-integral-to-regulated-mappings-eq-1} should be interpreted as a Riemann integral on $\real$, and \cref{eq:continuous-extension-integral-to-regulated-mappings-eq-2} is immediate:
    \begin{equation}\label{eq:continuous-extension-integral-to-regulated-mappings-eq-2}
        \int_a^b\vert f\vert \leq \abs{b-a}\norm{f}_u
    \end{equation}
    Next, let $\{f_n\}_{n\geq 1}$ be a sequence of step mappings in $I$ which converges uniformly to $f\in \cl{\Sigma}$. \Cref{eq:continuous-extension-integral-to-regulated-mappings-eq-2} tells us the sequence of integrals is uniformly Cauchy, as
    \begin{equation}\label{eq:continuous-extension-integral-to-regulated-mappings-eq-3}
        \abs{\int_a^bf_m - \int_a^b f_n}\leq \abs{b-a}\norm{f_m - f_n}_u
    \end{equation}
    Hence $\int_a^b f$ is well defined, \cref{eq:regulated-mapping-integral-continuous} and the properties listed in \cref{prop:properties-integral-step-mappings} follow upon taking limits.
\end{proof}
\begin{wts}[Integration and clms]\label{prop:regulated-mappings-integration-clms}
    Let $E$ and $F$ be Banach spaces, and $\lambda\in L(E,F)$. For a fixed interval $I$, denote the space of step mappings from $I$ to $E$ (resp. $F$) by $\Sigma_E$ (resp. $\Sigma_F$), and regulated mappings similarly. If $\{f_n\}\subseteq \Sigma_E$ converges uniformly to $f\in \cl{\Sigma_E}$, then $\{\lambda f_n\}\to \lambda f$ uniformly in $\cl{\Sigma_F}$. Moreover,
    \begin{equation}\label{eq:regulated-mappings-integration-clms}
    \lambda\qty(\int_a^b f) = \int_a^b\lambda f
    \end{equation}
\end{wts}
\begin{proof}
    The map $\lambda$ is Lipschitz between $E$ and $F$, and it descends into a map between the vector spaces $\Sigma_E$ and $\Sigma_F$ by composition. If $f$ is a step mapping, and $f\vert_{\cell(p_i)} = v_i$ for $i = \underline{k}$; the composition of $f$ with $\lambda$ is again a step mapping $\lambda f\vert_{\cell{p_i}} = \lambda v_i$.\\
    
    It is not hard to see $\norm{\lambda f}_u \leq \abs{\lambda}\norm{f}_u$, and 
    \begin{itemize}
        \item $\lambda$ is Lipschitz between $E$ and $F$,
        \item $\lambda$, when viewed as a map between $\Sigma_E$ and $\Sigma_F$, is Lipschitz.
    \end{itemize}
    Computing the integral of $\lambda f\in \Sigma_F$,
    \[
        \int_a^b \lambda f = \sum_{i=\underline{k}}\abs{\cell(p_i)}\lambda v_i = \lambda\qty(\sum_{i=\underline{k}}\abs{\cell(p_i)} v_i) = \lambda \int_a^b f
    \]
    proves \cref{eq:regulated-mappings-integration-clms} for step mappings, and the general case follows from continuity.
\end{proof}
\topheader{Fundamental Theorem of Calculus}
\begin{wts}
    Let $I$ be a compact interval, and $f: I\to E$ be regulated. Defining $\varphi:I \to E$ as the \emph{integral of $f$ with basepoint $a$}
    \begin{equation}\label{eq:ftc-phi-def}
        \varphi(t) = \int_a^t f
    \end{equation}
    Then $\varphi$ is differentiable where $f$ is continuous, and if $t_0\in I$ is such a point:
    \begin{equation}\label{eq:ftc-1}
        (D\varphi)(t_0) = f(t_0)
    \end{equation}
\end{wts}
\begin{remark}[Identifications]
    The left hand side in \cref{eq:ftc-1} should be thought of as a clm in $L(\real, E)$. We identify the point $f(t_0)$ as the map $t\mapsto t\cdot f(t_0)$.
\end{remark}

% This is similar to the time-one map of the exponential, is there a connection?
% The tangent space at the identity = the exponential.

\begin{proof}
    Suppose $f$ is continuous at $t_0$. For all $h$ sufficiently small, set $\varepsilon(h) = \sup_{{\abs{t-t_0}\leq h, \: t\in I}}\abs{f(t) - f(t_0)}$ as the modulus of continuity; where $\varepsilon(h)\to 0$ as $h\to 0$. Applying the well-known technique of estimating the integrand $f(t) = [f(t) - f(t_0)] + f(t_0)$, we have
    \begin{align}
        \varphi(t_0 + h) - \varphi(t_0) &= \int_{t_0}^{t_0 + h}f(t) dt\nonumber\\
        &= f(t_0)\cdot h + \int_{t_0}^{t_0 + h}[f(t) - f(t_0)]dt\label{eq:ftc-draft-1}
    \end{align}
    The last term within \cref{eq:ftc-draft-1} is $o(h)$, and the proof is complete.
\end{proof}
\topheader{Mean value theorems}
If $\lambda\in L(E,F)$, and $x\in E$, we write $\lambda \dot x = x\dot \lambda$. If $t\in \real$, and we want to think of $x$ as the map $t\mapsto tx$, we will write $t\cdot x = x\cdot t = tx$ to emphasize the role that $x$ plays. The duality pairing between $L(E,F)\times E\to F$ is bilinear and continuous. For any regulated mapping $\alpha: I\to L(E,F)$, 
\begin{equation}\label{eq:regulated-mapping-application}
    \int_a^b \alpha(t)\cdot x dt = \qty(\int_a^b \alpha(t) dt)\cdot x
\end{equation}
Furthermore, if $f\in C^1(I, E)$, we use the notation $f'(t)$ to refer to $Df(t)$; and we identify $f'(t)$ with an element in $E$; while $Df(t)$ should be thought of as a mapping in $L(\real,E)$.
%
\begin{lemma}[Constant curves]\label{lem:vanishing-derivative-implies-constant}
    If $\alpha\in C^1(I,E)$, $\alpha'=0$, iff $\alpha$ is constant.
\end{lemma}
\begin{proof}
    Suppose $\alpha'$ vanishes, and assume for contradiction there exists points $t_0< t_1$ in $I$ such that $\alpha(t_0)\neq \alpha(t_1)$. Hahn Banach gives us a clf $\lambda\in L(E,\real)$ that strictly separates the two points. See \cref{prop:hahn-banach-geometric-form} for a refresher. The ordinary derivative of $\lambda\circ f$ is $0$ everywhere which implies $\lambda\circ f$ is constant. The converse is trivial.
\end{proof}
\begin{lemma}[FTC 2]\label{lem:ftc-2}
    Let $f\in C^1(I, E)$, then
    \begin{equation}\label{eq:ftc-2}
        f(b) - f(a) = \int_a^b f'(t)dt    
    \end{equation}
    where the integrand in \cref{eq:ftc-2} is --- rigourously speaking --- a map  $\real\to L(\real,E)$, but we treat $f'(t)\in E$.
\end{lemma}
\begin{proof}
    Throughout this proof, we will treat $f': \real\to E$. Because $f'$ is continuous everywhere, it is regulated. Define $\varphi(t) = \int_a^t f'(t)dt$, by \cref{eq:ftc-1}:
    \[
        \varphi'(t) - f'(t)\equiv 0
    \]
    By \cref{lem:vanishing-derivative-implies-constant}, it suffices to show $(\varphi - f)(t) = f(a)$ at any point $t\in [a,b]$. Take $t = a$, and $(\varphi(a) - f(a)) = 0$, so that 
    \[
    \varphi(t) = f(t) + f(a)
    \]
    and \cref{eq:ftc-2} follows.
\end{proof}
\begin{remark}[Usefulness of FTC 2]
    \cref{lem:ftc-2} is most useful when $[a,b] = [0,1]$, and the $f$ is a curve interpolating between a $C^1$ function evaluated two different points, as in \cref{prop:mvt-1}.
\end{remark}
%
\begin{wts}[MVT 1]\label{prop:mvt-1}
    Let $U\osub E$ and $x\in U$, $y\in E$. If the line segment $L = \{x + ty,\: 0\leq t\leq 1\}$ is also contained in $U$ (draw a picture), then \cref{eq:mvt-1} holds.
    \begin{equation}\label{eq:mvt-1}
        f(x+y) = f(x) + \int_0^1 Df(x+ty)ydt = \qty(\int_0^1 Df(x+ty)dt)\cdot y
    \end{equation}
\end{wts}
\begin{proof}
    The curve $g(t) = f(x+ty)$ is composed of $f\circ l(t)$, for $l(t) = x+ty$. It has derivative
    \[
        g'(t) = Df(x+ty)\circ l'(t) = Df(x+ty)\circ (y\in L(\real,E))
    \]
    By \cref{lem:ftc-2}, $g(1)- g(0) =\int_0^1 Df(x+ty)\cdot y dt$. Given $g(1)-g(0) = f(x+y) - f(x)$, the proof is complete.
\end{proof}

\fchapter{4: Higher order derivatives}\newpage
\topheader{Introduction}
We start with the definition of $C^p(E,F)$. Let $E$ and $F$ be Banach Spaces, if $p\geq 1$ is an integer, we define the class $C^p$ to be the set of maps which are $p$ times differentiable, and $D^p f\in C(E, X)$, where 

\[
    X = L(E, L(E, L(E,\cdots F))  p\text{ times }  \Isomor{L} L(E^p, F)
\]

Sometimes we replace $E$ with an open subset $U\subseteq E$ if necessary, and we write $f\in C(U,F)$ if $D^p\in C(U,X)$. Note, even if $f\in C^1(U,F)$, $Df$ is still a map from $U$ into $L(E,F)$. 

We will prove two major results in this section.

\begin{itemize}
    \item The structure of the derivative $D^p f$, in particular, if $f\in C^p(E,F)$, then $D^pf(x)$ is a \emph{symmetric multilinear map} in $p$ arguments. 
    \item Taylor's Theorem
\end{itemize}

\topheader{The second derivative}
\begin{wts}[Product rule in $2$ variables]
    Let $E_1$, $E_2$ and $F$ be Banach spaces, if $\omega: E_1\times E_2\to F$ is bilinear and continuous, then $\omega$ is differentiable, and for every $(x_1, x_2)\in E_1\times E_2$, $(v_1,v_2)\in E_1\times E_2$,

    \[
        D\omega(x_1,x_2)(v_1,v_2) = \omega(x_1, v_2) + \omega(v_1, x_2)
    \]

    Furthermore, $D^2\omega(x,y) = D\omega\in L(E^2,F)$, and $D^3\omega = 0$.
    
\end{wts}
\begin{proof}
    By the definition of $\omega$, using the familiar interpolation method
    \[
        \omega(x_1 + h_2, x_2 + h_2) = \omega(x_1, x_2)  + \omega(x_1, h_2) + \omega(h_1, x_2) + \omega(h_1, h_2)
    \]
    by continuity of $\omega$, the last term (which we wish to make $o(h)$): 
    \[
    \vert \omega(h_1, h_2)\vert\leq \norm{\omega}\cdot\vert (h_1, h_2)\vert^2
    \]
    so that $\omega(h_1, h_2) = o(h)$, and $D\omega(x_1, x_2)$ exists and is continuous, and is given by the \emph{linear map} $\omega(x_1, \cdot) + \omega(\cdot, x_2)$. The rest of the proof follows, if it is not immediately obvious then read the following note.
    \begin{note}
        Write $E = E_1\times E_2$ for convenience. The linear map $A = D\omega(x_1, x_2)$ takes arguments $E$ into $F$, consider the projections $\pi_1$ and $\pi_2$, and $v\in E_1\times E_2$, then
        \[A(v) = \omega(x_1, \pi_1 v) + \omega(\pi_2 v, x_2)\]
        We can view $A(x) = D\omega(x_1, x_2)\in L(E,F)$. It is clear that $A$ is linear in $x$, if we fix $v\in E$, 
        \[
            A(x+y, v) = \omega(\pi_1(x+y), \pi_2 v) + \omega(\pi_1 v, \pi_2 (x+y)) = A(x,v) + A(y,v)
        \]
        and similarly for scalar multiplication. Hence $DA(x) = A\in L(E, L(E,F))$ and $D^2A(x) = D^3\omega = 0$.
    \end{note}
\end{proof}

Our next result is the following, which states that if $f: U\to F$ where $U\osub E$, and $Df, DDf = D^2f$ exists and are continuous maps from $U$ into $L(E,F)$ and $L(E,L(E,F)$ respectively, then $D^2f(x)$ is a \emph{symmetric bilinear map}. The proof is non-trivial, and relies on computing the 'Lie Bracket':

\[
    D^2f(x)(v,w) - D^2f(x)(w,v)
\]

Which we will prove is equal to $0$ for every $x\in U$, and $v,w\in E$.

\begin{wts}[Second derivative is symmetric]
    Let $f\in C^2(U, F)$, where $U\osub E$ with the possibility that $U = E$. For every point $x\in U$, the \emph{second derivative} $D^2f(x)$ is bilinear and symmetric.
\end{wts}
\begin{proof}
    Fix $x\in U\induces B(r) + x\osub U$. We restrict our attention to vectors $v,w\in E$ where $\vert v\vert, \vert w\vert < r2^{-1}$ for now, so that the
    \[
    \bigset{x, x+w, x+v, x+v+w}\subseteq U
    \]
    We will denote the following quantity by $\Delta$
    \[
        \Delta = f(x+w+v) - f(x+w) - f(x+v) + f(x)
    \]
    By rearranging terms, we see that $\Delta$ can be approximated in two ways:
    \begin{itemize}
        \item Postponing the discussion about the the domain of $y$, set $g(y) = f(y+v) - f(y)$ is $C^2$, and 
        \begin{equation}\label{second-derivative-Delta-g}
            \Delta = g(x+w) - g(x)
        \end{equation}
        \item Again, for $y$ sufficiently close to $x$, define $h(y) = f(y+w) - f(y)$, and
        \begin{equation}\label{second-derivative-Delta-h}
            \Delta = h(x+v) - h(x)
        \end{equation}
        \item To find the domain for $y$, an easy argument using the Triangle inequality gives us $g,h \in C^2(B(r2^{-1}) + x, F)$,
        \item Leaving the computations of $h$ as an exercise, we compute $Dg$, recall the shift map $y\mapsto y+v$ commutes with $D$, and
        \begin{equation}\label{second-derivative-Delta-Dg}
            Dg(y) = D(\tau_{-w}f)(y)  - Df(y) = Df(y+w) - Df(y)
        \end{equation}
    \end{itemize}

    Using MVT twice, once on \Cref{second-derivative-Delta-g} (the line segment $x + tw$, $0\leq t\leq 1$ is contained in the domain of $g$), and another time on \Cref{second-derivative-Delta-Dg} (with $y = x+tw$ in the integrand). We obtain:
    \begin{align*}
        \Delta &= g(x+w) - g(x)= \int_{0}^1 Dg(x+tw)\cdot w dt\\[1ex]
        &= \int_0^1\int_0^1 D^2f(x + tw+sv)\cdot v ds\: dt \cdot w= \int_0^1\int_0^1 D^2f(x + tw + sv) ds\: dt \cdot v\cdot w
    \end{align*}
    % Justification for pulling out the duality pairing from the integral
    We can rewrite the application of $v$ then $w$ by $\cdot (v,w)$, and using the approximation $D^2f(x+tw+sv)\cdot(v,w) = D^2f(x)\cdot(v,w)+\delta_1(tw,sv)$. Integrating over $s,t$ gives
    \[
        \Delta = D^2f(x)\cdot(v,w) + \int_0^1\int_0^1 \delta_1(tw,sv) ds\: dt
    \]
    \begin{note}
        The error term $\delta_1$ in the integrand is given by
        \[
            \delta_1(tw,sv) = D^2f(x + tw + sv)(v,w) - D^2f(x)(v,w)
        \]
        for $v,w$ sufficiently small and $0 \leq s,t\leq 1$.
    \end{note}

    A similar argument for $h$ shows that $\Delta = D^2f(x)\cdot(w,v) + \int_0^1\int_0^1 \delta_2(tw,sv)ds\: dt$. Combining the two together, the following holds for all $v,w$ sufficiently small:
    \begin{equation}\label{second-derivative-lie-bracket}
        D^2f(x)\cdot (v,w) - D^2f(x)\cdot (w,v) = \int_0^1\int_0^1 \delta_1(tw,sv)ds\: dt - \int_0^1\int_0^1 \delta_2(tw,sv)ds\: dt
    \end{equation}

    To show the right hand side is $0$, we will need the following note. 

    \begin{note}
    We wish to show the RHS of \Cref{second-derivative-lie-bracket} is $0$. We begin by controlling the RHS and show that it is super-bilinear; meaning it shrinks after than the product $\abs{v}\abs{w}$. Then, we will prove a lemma which will show the only bilinear map that satisfies this property is the $0$ map.

    \begin{itemize}
        \item For $j=1,2$, relabel $\delta = \delta_j$ for convenience. We can use the $L^1$ inequality, to obtain the estimate
        \begin{equation}\label{second-derivative-step2}
            \abs{ \int_0^1\int_0^1\delta(tw,sv)ds\: dt} \leq \int_0^1\int_0^1\abs{\delta(tw,sv)}ds\: dt
        \end{equation}
        \item $\delta(tw,sv)$ is controlled by $\abs{D^2f(x + tw + sv) - D^2f(x)}\abs{v}\abs{w}$. Take $y = tw+sv$, then $\abs{y} \leq \abs{tw} + \abs{sv}$. Hence,
        \begin{equation}\label{second-derivative-control-on-rhs}
            \vert\delta_j\vert\leq\abs{D^2 f(x + tw + sv) - D^2f(x)}\abs{v}\abs{w}
        \end{equation}
        \item Let $A$ denote the span of $w,v$ for scalars $s,t\in[0,1]$. In symbols,
        \[
            A = \bigset{tw+sv,\: s,t\in [0,1]}
        \]
        $A$ is clearly compact, and the continuity of $D^2f$ means 
        
        \begin{equation}\label{second-derivative-R}
        R(v,w,\delta) = \sup_{y\in A}\abs{D^2f(x + y) - D^2f(x)}\quad\text{is finite},\qqtext{and}\lim_{(v,w)\to 0}R(v,w,\delta)=0
        \end{equation}
        See \cref{rmk:compact-linear-combination} for a generalization of this argument.
        \item Relabel $R(v,w)$ to be the maximum across $R(v,w,\delta_1)$ and $R(v,w,\delta_2)$. 
        \item Combining \Cref{second-derivative-step2,second-derivative-control-on-rhs,second-derivative-R}, we obtain the following bound on \Cref{second-derivative-lie-bracket}
        \begin{align}
            \abs{D^2f(x)\cdot(v,w) - D^2f(x)\cdot(w,v)} &\leq \abs{\iint\delta_1(tw,sv) ds\: dt - \iint\delta_2(tw,sv)ds\: dt}\nonumber\\
            &\leq \iint\abs{\delta_1}ds\: dt + \iint\abs{\delta_2}ds\: dt\nonumber \\
            &\leq \abs{v}\abs{w}R(v,w)\label{second-derivative-step3}
        \end{align}
    \end{itemize}
    The following Lemma gives a useful criterion to check when a multilinear map is identically $0$.
    \begin{lemma}
        Let $E$ be a Banach space, and $k\geq 1$ be an integer. If $\lambda\in L(E^k, F)$ and there exists another map $\theta: E^k\to F$ (defined perhaps on an open neighbourhood of the origin), such that
        \[
        \vert \lambda(\UL{u}[k])\vert \leq\vert\theta(\UL{u}[k])\vert\cdot\prod\vert \UL{u}[k]\vert
        \]
        for all $(\UL{u}[k])$ sufficiently small. And $\lim_{(\UL{u}[k]) \to 0}\theta(\UL{u}[k])=0$, then, $\lambda=0$.
    \end{lemma}
    \begin{proof}
        Fix arbitrary $(\UL{u}[k])\in E^k$, for $s>0$ sufficiently small, the left hand side of the equation reads
        \[
            \vert s\vert^k\vert\lambda (\UL{u}[k])\vert\leq\vert\theta(s\UL{u}[k])\vert\cdot\vert s\vert^k\prod \vert\UL{u}[k]\vert
        \]
        The rest of the argument is Archimedean: divide by $\vert s\vert^k$ and send $s\to 0$ (while paying attention to the term with $\theta$): perhaps after relabelling $v_s = s\UL{u}[k]$ for sufficiently small $s$, then $\vert \theta(v_s)\vert\to 0$ as $s\to 0$.
    \end{proof}
        
    \end{note}
    
    

\end{proof}
\begin{remark}[Compact linear combinations]\label{rmk:compact-linear-combination}
    Generalization of the "compact linear combination" argument used above. Let $(\UL{t}[k])\subseteq\mathbb{C}^k$ or $\real^k$, and vectors $\UL{v}[k]\in E$. Suppose further $(\UL{t}[k])\subseteq A$ is compact in $\mathbb{C}^k$ or $\realk$. It is clear that if $y = t_iv^i\in E$, where the summation convention is in effect. Then,
    \[
        \abs{y}\Lsim_A \abs{(v^{\underline{k}})}_{E^k}
    \]
    Now, fix a continuous function $f\in C(E,F)$, we can approximate the maximum error over all such $y$
    \[
        \sup_{y\in B}\abs{f(x+y) - f(x)}< \varepsilon\quad\forall \abs{y}\Lsim_A\vert(v^{\underline{k}})\vert < \delta
    \]
    where
    \[
    B = \bigset{\sum t_iv^i,\: (\UL{t}[k])\subseteq A,\: (v^{\underline{k}})\in E^k }
    \]
\end{remark}

\topheader{The $p$-th derivatives}
If $f$ is $p$ times differentiable, and $f, Df, D^2f, \ldots, D^pf$ are all continuous, then we say $f\in C^p(E,F)$ (replacing $E$ with an open subset of $E$ if necessary). 

\begin{wts}
    If $f\in C^p(E,F)$, then $D^pf(x)$ is symmetric for every $x\in E$. (Replace $E$ with an open set if necessary).
\end{wts}
\begin{proof}
    The main proof proceeds as follows. We will use induction on $p$, with $p=2$ serving as the base case. Our induction hypothesis is that for every $f\in C^{p-1}(E,F)$, for every permutation $\beta\in S_{p-1}$, at every point $x\in E$, for every possible choice of $p-1$ vectors $(v_2,\ldots, v_{p}) = (v_{1 + \underline{p-1}})$,
    \[
        D^{p-1}f(x)(v_{1+\underline{p-1}}) = D^{p-1}f(x)(v_{1+\beta(\underline{p-1})})
    \]
    To prove the assertion for $p$, it suffices to show $D^pf(x)(v_{\underline{p}})$ is invariant under transpositions of indices; since the transpositions generate $S_p$. Furthermore, the transpositions in $S_p$ are generated by 
    
    \begin{itemize}
        \item the transposition $(1,2,\ldots)\mapsto (2,1,\ldots)$ where the omitted indices are held fixed, and
        \item the transpositions which leave the first index fixed:
        \[
            (1,1+\underline{p-1})\mapsto (1,1+\beta({\underline{p-1}}))
        \]
        where $\beta\in S_{p-1}$
    \end{itemize}

    so it suffices to prove invariance under those two types of transpositions. Let $g = D^{p-2}f$, so $g\in C^2(E, L(E^{p-2}, F))$. Because the application of vectors (currying) on a multilinear map $A\in L(E^p, F)$ is associative, illustrated as follows:
    \[
       (A\cdot v_1)\cdot v_2 = A\cdot (v_1,v_2) = A(v_1, v_2,\cdot)\in L(E^{p-2}, F)
    \]
    Then, let $\lambda: L(E^{p-2}, F)\to F$ be the evaluation map at $(v_3,\ldots, v_p) = (v_{2+\underline{p-2}})$. 
    Using the base case on $D^{p-2}f = g\in C^2(E, L(E^{p-2}, F))$, 
    \[
        (D^2g)(x)(v_1,v_2) = (D^2g)(x)(v_2,v_1)\implies \lambda\qty\Big((D^2g)(x)(v_1,v_2)) = \lambda\qty\Big((D^2g)(x)(v_2,v_1))
    \]
    But $\lambda$ is the map that \emph{applies} the rest of the vectors, and
    \begin{equation}\label{second-derivative-D2g-equality}
        (D^2g)(x)(v_1,v_2)\cdot (v_{2+\underline{p-2}}) = (D^2g)(x)(v_2,v_1)\cdot (v_{2+\underline{p-2}})
    \end{equation}
    Since $D$ commutes with continuous linear maps (and $\lambda$ is continuous because $(v_{2+\underline{p-2}})$ is fixed),
    \begin{equation}\label{second-derivative-application-map-lambda-commute}
       \lambda(D^2(D^{p-2}f)) = D(\lambda(D(D^{p-2}f)) = D(D\lambda\circ D^{p-2}f) = D^2(\lambda\circ D^{p-2}f) 
    \end{equation}
    Substituting \Cref{second-derivative-D2g-equality} for the rightmost hand side of \Cref{second-derivative-application-map-lambda-commute} gives the result.
    \begin{note}
        There are no magic 'identifications' being made here. To be perfectly clear, for each $x\in E$, $g(x)$ is an element in $L(E^{p-2}, F)$, and $(D^2g)(x)\in L(E^2, L(E^{p-2}, F))$. Evaluating $g$ at a point $x$ gives a bilinear map that takes values in the Banach space $L(E^{p-2}, F)$.
    \end{note}
    For the second case, beginning from the induction hypothesis. If $\theta$ is a $p$-permutation that leaves the first coordinate unchanged, then there exists a unique $p-1$-permutation $\beta\in S_{p-1}$ such that
    \begin{align}\label{multi-derivatives-theta-def}
    \qty\big(\:\theta(\underline{p})\:) &= \qty\big(1,\theta(1+\underline{p-1}))\nonumber\\
    &= \qty\big(1,1+\beta(\underline{p-1}))    
    \end{align}
    Using a similar argument as the first case, set $g = D^{p-1}f$ and $\lambda, \lambda' \in L(E^{p-1}, F)$ to be the evaluation maps of $(v_1, v_{1+\underline{p-1}}) = (v_{\underline{p}})$ and $(v_1, v_{1+\beta(\underline{p-1})})$ respectively. Rehearsing the same proof as before:
    \begin{align*}
        (D^pf)(x)(v_{\underline{p}}) &= D\qty\big(\lambda D^{p-1}f)(x)(v_1)&&\text{\Cref{second-derivative-application-map-lambda-commute}}\\
        &= D\qty\big(\lambda' D^{p-1}f)(x)(v_1) &&\text{ind. hyp.}\\
        &= (D^pf)(x)(v_{\theta(\underline{p})})&&\text{\Cref{second-derivative-application-map-lambda-commute}}
    \end{align*}
    This proves the induction step, and the proof is complete.
\end{proof}
Before stating and proving Taylor's Theorem, an important remark on the 'postcomposition' of linear maps. Summarized in the following note. 

\begin{note}
    Let $f\in C^p(E,F)$, and $\lambda\in L^p(F,G)$. $\lambda$ induces a map between $L(E^p, F)$ and $L(E^p, G)$ by post-composing any multi-linear map $A\in L(E^p, F)$ by $\lambda$. Denoting this map by $\lambda_*$, 
\[
\lambda_*: L(E^p, F)\to L(E^p, G)
\]
It is clear $\lambda_*$ is linear and continuous. And its action on $A$, evaluated at $(v_{\underline{p}})\in E^p$ is given by
\[
    \lambda_*(A)\in L(E^p, G)\quad \qty\big(\lambda_*(A))(v_{\underline{p}}) = \lambda\qty\big(A(v_{\underline{p}})) = (\lambda\circ A)(v_{\underline{p}})
\]
Now, recall that for $p=1$
\[
    \qty\big[D(\lambda\circ f)](x) = \lambda\qty\big[(Df)(x)]
\]
To simplify the notation, we want to 'move' the evaluation $x$ outside of the brackets, and somehow write $x\mapsto \lambda\qty\big[(Df)(x)]$ as one map between $E$ and $L(E,G)$. We further \emph{identify} $\lambda$ as this map, so that
\[
    \qty\big[D(\lambda\circ f)](x) = \lambda  = \qty\big(\lambda \circ Df)(x)
\]
Dropping the $x$ from the expression, for $p\geq 2$ \emph{assuming a similar formula holds}, then we write $\qty\big[D^p(\lambda\circ f)] = \lambda_*\circ D^pf$. We make a final identification, of $\lambda = \lambda_*$ (thereby conflating the two different maps, the first is a map from $E$ to $F$, the second is a map from $L(E^p, F)$ into $L(E^p, G)$).
\end{note}

\begin{wts}[CLMs commute past $D^p$]\label{prop:linear-maps-commute-with-p-derivative}
    If $p\geq 2$, $f\in C^p(E,F)$, $\lambda\in L(F,G)$, then
    \[
        D^p(\lambda\circ f) = \lambda\circ D^pf
    \]
    Where we have identified $\lambda$ as the same map that acts on $L(E^p, F)$ to produce another map in $L(E^p, G)$, and suppressed the point $x$.
\end{wts}
\begin{proof}
    Use induction on $p$.
\end{proof}
\begin{wts}[$C^p$ is closed under composition]\label{prop:Cp-banach-closed-under-composition}
    If $f\in C^p(E,F)$, and $g\in C^p(F,G)$, then $g\circ f\in C^p(E,G)$.
\end{wts}
\begin{proof}
    Postponed.
\end{proof}
\begin{wts}[Taylor's Formula]\label{prop:taylors-formula}
    Let $f\in C^p(U, F)$, where $U\osub E$. For $x\in U$ and $y\in E$ such that $L = \{x + ty,\: 0\leq t\leq 1\}$ is in contained in $U$, then 
    \begin{equation}\label{eq:taylors-formula-1}
        f(x+y) = f(x) + \qty(\sum_{i=\underline{p-1}}\dfrac{D^if(x)\cdot (y^{(i)})}{(p-1)!}) + R_p
    \end{equation}
    where $\cdot (y^{(i)})$ denotes the consecutive application of $y$ for $i$ times. The remainder $R_p$ is given by \cref{eq:taylors-formula-2}
    \begin{equation}\label{eq:taylors-formula-2}
        R_p = \int_0^1 \dfrac{(1-t)^{p-1}}{(p-1)!}D^p f(x+ty)dt\cdot (y^{(p)})
    \end{equation}
    Furthermore, we include the $p$th term in the series using \cref{eq:taylors-formula-3}
    \begin{equation}\label{eq:taylors-formula-3}
        f(x+y) = f(x) +\sum_{i=\underline{p}}\dfrac{D^if(x)\cdot (y^{(i)})}{i!} + \theta(y)
    \end{equation}
    where $\theta$ is defined for small $y$, and $o(\vert y\vert^p)$. 
    \begin{equation}\label{eq:taylors-formula-4}
        \abs{\theta(y)}\leq\sup_{0\leq t\leq 1}\dfrac{\abs{D^p f(x+ty) - D^p f(x)}}{p!}\abs{y}^p
    \end{equation}
\end{wts}
\begin{proof}
    Postponed.
\end{proof}
\end{document}

